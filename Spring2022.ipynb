{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting to reveal up to 500 rows in notebook\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence pink warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(a_df):\n",
    "    '''Performs initial cleaning of dataframe'''\n",
    "    \n",
    "    a_df = a_df.drop_duplicates(keep=\"first\")\n",
    "    \n",
    "    return a_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_caps(a_df, a_list):\n",
    "    '''Renders string df columns in supplied list in all caps, returns dataframe'''\n",
    "    \n",
    "    for col in a_list:\n",
    "        a_df[col] = a_df[col].str.upper()\n",
    "        \n",
    "    return a_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vc(a_ser):\n",
    "    '''Return value_counts().to_frame() for a series'''\n",
    "    \n",
    "    return a_ser.value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_isbns(a_ser):\n",
    "    '''Renders isbn series data as str and strips unwanted chars, returns a series'''\n",
    "    \n",
    "    a_ser = a_ser.astype(\"str\") \n",
    "    a_ser = a_ser.str.rstrip(\".0\")\n",
    "    \n",
    "    return a_ser    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prices(a_ser):\n",
    "    '''Strips unwanted chars from price data and renders as float, returns a series'''\n",
    "    \n",
    "    a_ser = a_ser.astype(\"str\") \n",
    "    a_ser = a_ser.str.lstrip(\"$\")\n",
    "    a_ser = a_ser.str.replace(\",\", \"\", regex=False)\n",
    "    a_ser = a_ser.str.replace(\"PRICE NOT YET AVAILABLE**\", \"0.01\", regex=False)\n",
    "    a_ser = a_ser.str.replace(\"PREPAID\", \"0.01\", regex=False)\n",
    "    a_ser = a_ser.apply(lambda x: float(x))\n",
    "\n",
    "    return a_ser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import courses_#.csv files into dataframe, clean and transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *courses_df*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV files from List\n",
    "\n",
    "csvs_files = [\"Resources\\\\courses_1.csv\", \"Resources\\\\courses_2.csv\", \"Resources\\\\courses_3.csv\"]\n",
    "\n",
    "course_csv_list = (pd.read_csv(file) for file in csvs_files)\n",
    "\n",
    "df1 = pd.concat(course_csv_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking shape and dtypes of data\n",
    "\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate dupes and drop unneeded columns\n",
    "\n",
    "df1 = df1.drop_duplicates()\n",
    "\n",
    "df1 = df1.drop(columns=[\"Unnamed: 0\", \"store_id\", \"catalog_id\", \"campus\", \"campus_id\", \"term_id\", \"scanDate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns and capitalize data where appropriate\n",
    "\n",
    "df1 = df1[[\"department_id\", \"course_id\", \"section_id\", \"university\", \"term\", \"department\", \"course\", \"section\"]]\n",
    "\n",
    "all_caps(df1, [\"university\", \"term\", \"department\", \"course\", \"section\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine value counts on term column to begin consolidation into categories \n",
    "\n",
    "vc(df1[\"term\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use boolean mask to consolidate Spring 2022 term data \n",
    "\n",
    "spring_mask = df1[\"term\"].str.contains(\"SPRING\")\n",
    "\n",
    "df1.loc[spring_mask, \"term\"] = \"SPRING\"\n",
    "\n",
    "\n",
    "fall_mask = df1[\"term\"].str.contains(\"FALL\")\n",
    "\n",
    "df1.loc[fall_mask, \"term\"] = \"FALL\"\n",
    "\n",
    "\n",
    "summer_mask = df1[\"term\"].str.contains(\"SUMMER\")\n",
    "\n",
    "df1.loc[summer_mask, \"term\"] = \"SUMMER\"\n",
    "\n",
    "\n",
    "winter_mask = df1[\"term\"].str.contains(\"WINTER\")\n",
    "\n",
    "df1.loc[winter_mask, \"term\"] = \"WINTER\"\n",
    "\n",
    "\n",
    "\n",
    "junk_mask = ~(df1[\"term\"].isin([\"SPRING\", \"FALL\", \"SUMMER\", \"WINTER\"]))\n",
    "\n",
    "df1.loc[junk_mask, \"term\"] = \"JUNK\"\n",
    "\n",
    "df1 = df1.loc[~(df1[\"term\"]==\"JUNK\"), :]\n",
    "\n",
    "\n",
    "vc(df1[\"term\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_df = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import textbooks_#.csv files into dataframe, clean and transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *textbooks_df*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV files from List\n",
    "\n",
    "csvs_files = [\"Resources\\\\textbooks_1.csv\", \"Resources\\\\textbooks_2.csv\", \"Resources\\\\textbooks_3.csv\",\\\n",
    "              \"Resources\\\\textbooks_4.csv\", \"Resources\\\\textbooks_5.csv\"]\n",
    "\n",
    "course_csv_list = (pd.read_csv(file) for file in csvs_files)\n",
    "\n",
    "df2 = pd.concat(course_csv_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking shape and dtypes of data\n",
    "\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate dupes and drop unneeded columns\n",
    "\n",
    "df2 = df2.drop_duplicates()\n",
    "\n",
    "df2 = df2.drop(columns=\\\n",
    "    [\"Unnamed: 0\", \"store_id\", \"catalog_id\", \"campus_id\", \"term_id\", \"book_id\",\\\n",
    "     \"no_textbook_message\", \"recommend_type\", \"scanDate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns and capitalize data where appropriate\n",
    "\n",
    "all_caps(df2, [\"title\", \"edition\", \"publisher\", \"book_type\", \"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows where title, ISBN, and price are ALL missing (i.e., not much use for analysis)\n",
    "\n",
    "df2 = df2.dropna(axis=0, subset=[\"title\", \"ISBN\", \"price\"], how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values with default values  \n",
    "\n",
    "df2 = df2.fillna(value={\"edition\":\"unknown\", \"publisher\":\"unknown\", \"ISBN\": 0.0, \"price\":\"0.01\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename ISBN column to isbn \n",
    "\n",
    "df2 = df2.rename(columns={\"ISBN\":\"isbn\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform isbn data\n",
    "\n",
    "df2[\"isbn\"] = clean_isbns(df2[\"isbn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform price data\n",
    "\n",
    "df2[\"price\"] = clean_prices(df2[\"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random sample of our dataframe to see if transformations are effective\n",
    "\n",
    "df2.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textbooks_df = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textbooks_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textbooks_df.sample(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge courses and textbooks dataframes with inner join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *merge_courses_and_textbooks_df*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner merge on both dataframes on common id fields\n",
    "\n",
    "merge_df = pd.merge(courses_df, textbooks_df, on=[\"department_id\", \"course_id\", \"section_id\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = merge_df[[\"university\", \"term\", \"department\", \"course\", \"section\", \"title\", \"edition\", \"price\",\\\n",
    "                     \"isbn\", \"publisher\", \"book_type\", \"book_url\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = merge_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_textbooks = merge_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render as csv file for import into PostGresSQL\n",
    "\n",
    "course_textbooks.to_csv(\"Resources\\\\course_textbooks.csv\", encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
