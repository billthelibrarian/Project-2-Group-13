{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Report\n",
    "#### Collegiate Courses - New Textbook Library and Bookstore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "* We're taking raw data, extracting the information we want, transforming the information so it is in a universal format, and loading into a more user friendly database so analysts can more easily read and present the data. We set a goal of providing this data in **2nd normal form** for our analysts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract\n",
    "\n",
    "* We downloaded two csv files comprising a dataset on kaggle.com, one with textbook data `BNTextbook_2022-02-05.csv | 278 MB`, and another with college course data `BNCollegeCourses_2022-02-05.csv | 97 MB`. Due to the enormity of these files, we used pandas to partition these csv files into smaller, GitHub-friendly csv files. \n",
    "\n",
    "\n",
    "* We loaded each large csv into a pandas dataframe and determined its total number of rows. We decided that each smaller csv should have no more than 200,000 rows. Our partitioning code thus rendered 5 textbook data csvs and 3 course data csvs in the `/Resources` folder.  \n",
    "\n",
    "\n",
    "* `courses_1.csv   | 33 MB`\n",
    "* `courses_2.csv   | 32 MB`\n",
    "* `courses_3.csv   | 24 MB`\n",
    "\n",
    "* `textbooks_1.csv | 49 MB`\n",
    "* `textbooks_2.csv | 53 MB`\n",
    "* `textbooks_3.csv | 51 MB`\n",
    "* `textbooks_4.csv | 50 MB`\n",
    "* `textbooks_5.csv | 54 MB`\n",
    "\n",
    "\n",
    "* See `Partition_CSVs.ipynb` for the code which rendered these smaller csvs into the `/Resources` folder.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform\n",
    "\n",
    "\n",
    "* After loading the smaller csv files into their respective dataframes in pandas, we applied custom transformations as follows:\n",
    "\n",
    "**courses data**\n",
    "\n",
    "1. starting dataframe: `537821 rows × 15 columns`\n",
    "2. dropped duplicate rows and unneeded columns\n",
    "3. rendered textual data in all caps for readability\n",
    "4. applied boolean masks to render `term` column data into 4 categories: (FALL, WINTER, SPRING, SUMMER)\n",
    "5. reindexed dataframe\n",
    "6. transformed dataframe: `523968 rows × 8 columns`\n",
    "\n",
    "\n",
    "**textbooks data**\n",
    "\n",
    "1. starting dataframe: `990727 rows × 19 columns`\n",
    "2. dropped duplicate rows and unneeded columns\n",
    "3. rendered textual data in all caps for readability\n",
    "4. dropped rows where `title, ISBN, price` columns were _all_ missing data\n",
    "5. filled missing data in `edition, publisher` columns with \"unknown\" \n",
    "6. filled missing data in `ISBN` column with 0.0\n",
    "7. filled missing data in `price` column with \"0.01\"\n",
    "8. converted `ISBN` column from float to string\n",
    "9. removed trailing \".0\" from `ISBN` column data\n",
    "10. renamed `ISBN` column to `isbn`\n",
    "11. removed leading dollar signs, commas, and other string literals from `price` column data\n",
    "12. converted `price` column from string to float so analysts can perform math in the database\n",
    "13. reindexed dataframe\n",
    "14. transformed dataframe: `645255 rows × 10 columns`\n",
    "\n",
    "**merged courses and textbooks data**\n",
    "\n",
    "1. inner-join merged the transformed dataframes above on 3 common columns: (`department_id, course_id, section_id`)\n",
    "2. dropped duplicate rows and unneeded columns\n",
    "3. reindexed dataframe\n",
    "4. transformed dataframe: `610343 rows × 12 columns`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load\n",
    "\n",
    "* We used PostgreSQL to upload our data into a database called `library_db`\n",
    "* We then created file `populate.sql` to create a table `course_textbook`\n",
    "* Lastly, we created an engine with SqlAlchemy that connects pandas dataframe to append the sql table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1445c6c1a0e1268b1047d35eededca9512247ec515644c5d756f4d61d21fe4b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
